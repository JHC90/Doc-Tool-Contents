{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''''\n",
    "{\n",
    "\"title\": \"Data-Preprocessing-Checklist\",\n",
    "\"keywords\": \"DataPreprocessing, \",\n",
    "\"categories\": \"\",\n",
    "\"description\": \"Hier geht es um die individuelle Abarbeitung der <em>EDA</em>-Checkliste im Kontext der California-Housing Problematik\",\n",
    "\"level\": \"30\",\n",
    "\"pageID\": \"16112020-10-California-Housing-Data-Preprocessing-Checklist\"\n",
    "}\n",
    "'''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>California Housing <br> Data Preprocessing</h1></center>\n",
    "\n",
    "![](imgs/2020-11-14-21-31-19.png)\n",
    "\n",
    "In diesem Notebook wird die [EDA-Checkliste](16112020-EDA-Checklite) im Kontext des California-Housing Problem abgeabreitet. Bei dieser Checkliste geht es darum, die Datenstruktur grundlegend zu verstehen. Hier werden die Daten lediglich beschrieben, das ist wichtig bevor die Modellierung startet.\n",
    "\n",
    "# Laden der Daten\n",
    "Die Daten wurden bereits im vorherigen [Notebook - California Housing Priceses Data](14112020-10-California-Housing-Data) gesplittet und persistent auf der Festplatte gespeichert. Somit müssen diese Daten in diesem Notebook zunächst geladen werden. Die Funktion Daten-Laden wurde in diesem Notebook der übersichtlichkeit wegen ebenfalls ausgelagert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\images\\end_to_end_project\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import FunctionFileCalifornia as ffc\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic Variables\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"end_to_end_project\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = ffc.load_housing_data()\n",
    "strat_test_set = ffc.load_housing_data(filename=\"strat_test_set.csv\")\n",
    "strat_train_set = ffc.load_housing_data(filename=\"strat_train_set.csv\")\n",
    "#print(housing.shape, strat_test_set.shape, strat_train_set.shape)\n",
    "#print(strat_test_set.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertikaler Cut\n",
    "Im vorherigen [horizontalen Cut](14112020-10-California-Housing-Data) wurden die Trainings und Testdaten erstellt und auf der Festplatte gepseichert. In dem Vertikalen Cut werden nun die Trainings und Testdaten in jeweils beschreibende und zu erklärende Variable aufgeteilt = Supervised Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 12)\n",
      "(16512, 11)\n",
      "(16512,)\n"
     ]
    }
   ],
   "source": [
    "print(strat_train_set.shape)\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "print(housing.shape)\n",
    "print(housing_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1. Data-Cleaning - NAN-Values](07112020200718-DataCleaning)\n",
    "Aus dem [Data-Management-Notebook](14112020-10-California-Housing-Data) ist klar, dass NAN/0/\"\" Werte in dem Feature \"total bedrooms\" existieren. Diese Werte werden nun in dieser Rubrik behandelt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.isnull().any())\n",
    "print(housing.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "In dieser Lösung entscheide ich mich dazu eine Imputaion durchzuführen. Ander Lösungen finden sich in diesem [Notebook](16112020-NAWerte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\n",
    "except ImportError:\n",
    "    from sklearn.preprocessing import Imputer as SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# erstellen eines Sub-DF welches nur die numerischen Werte beinhaltet\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "imputer.fit(housing_num)\n",
    "# imputer.statistics_\n",
    "# housing_num.median().values\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_tr.shape)\n",
    "#print(housing_tr.isnull().any())\n",
    "\n",
    "print(housing.shape)\n",
    "#print(housing.isnull().any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2. Feature-Selection](07112020200718-FeatureSelection)\n",
    "Im konkreten Beispiel verwende ich alle gegebenen Feature des Datensatzes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3. Feature-Engineering](07112020200718-FeatureEngineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "für die kategorischen Variablen check out das [OHE-Notebook](16112020-OneHotEncodingOrdinalEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "except ImportError:\n",
    "    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.shape\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "housing_ocean_proximity_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "titles = cat_encoder.get_feature_names(['ocean_proximity'])\n",
    "partOHEdf = pd.DataFrame(housing_ocean_proximity_cat_1hot, columns=titles)\n",
    "print(partOHEdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenfügen des OHE & Imputer DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingDF = pd.concat([housing_tr,partOHEdf],axis=1)\n",
    "print(housingDF.shape)\n",
    "print(housingDF.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
    "    list(housing.columns).index(col)\n",
    "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_extra_attribs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4. Feature Scaling](07112020200718-FeatureScaling)\n",
    "wichtig ist, dass die Skalierungen später(nach den Predictions) wieder zurück skaliert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#print(housing_tr)\n",
    "scaler.fit(housingDF)\n",
    "housing_tr_scaled = scaler.transform(housingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_tr_scaled.shape)\n",
    "#print(houhousingDF.colsing_tr_scaled)\n",
    "titles = housingDF.columns\n",
    "finalPreporcessedDF = pd.DataFrame(housing_tr_scaled, columns=titles)\n",
    "print(finalPreporcessedDF.shape)\n",
    "print(finalPreporcessedDF.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5. SK-Learn Pipeline](07112020200718-FeatureScaling)\n",
    "Die oberen Schritte waren bisher primär für die Entwicklung. Für einen vernünftigen Einsatz werden nun [SK-Learn Pipelines]() verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = ffc.load_housing_data()\n",
    "strat_test_set = ffc.load_housing_data(filename=\"strat_test_set.csv\")\n",
    "strat_train_set = ffc.load_housing_data(filename=\"strat_train_set.csv\")\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "housing_cat = [\"ocean_proximity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def add_extra_features(X, add_bedrooms_per_room=True):\n",
    "    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "    population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "    if add_bedrooms_per_room:\n",
    "        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "        return np.c_[X, rooms_per_household, population_per_household,\n",
    "                     bedrooms_per_room]\n",
    "    else:\n",
    "        return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = FunctionTransformer(add_extra_features, validate=False,\n",
    "                                 kw_args={\"add_bedrooms_per_room\": False})\n",
    "housing_extra_attribs = attr_adder.fit_transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_extra_attribs = pd.DataFrame(\n",
    "    housing_extra_attribs,\n",
    "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n",
    "    index=housing.index)\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('OHE', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_tr\n",
    "from pipe_tools.pipe_visualizer import plot_pipeline\n",
    "plot_pipeline(num_pipeline, \"pipeline_plot.png\")\n",
    "plot_pipeline(cat_pipeline, \"pipeline_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    print(\"SK-Learn Version passt\")\n",
    "except ImportError:\n",
    "    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20\n",
    "    print(\"Alte Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(housing_num)\n",
    "cat_attribs = list(housing_cat)\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "print(housing.shape)\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "print(housing_prepared.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
